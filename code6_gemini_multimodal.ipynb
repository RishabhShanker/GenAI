{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG3_LKsWSD3A"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    gcloud init\n",
        "\n",
        "step 1 - select mode : 2 (sign in with new google account)\n",
        "- it would launch a browser for you to eneter your google cloud credentials, \n",
        "- allow all, next, next, complete\n",
        "- step 2: select project: project 1 - bdc-training\n",
        "- step 3: region for VM: uscentral1-b\n",
        "\n",
        "\n",
        "        gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services.\n",
        "\n",
        "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymmEN6GSTn-"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    - [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    - Run the cell below to set your project ID and location.\n",
        "    - Read more about [Supported locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    - [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    - Run the cell further below to use your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1933326c939"
      },
      "source": [
        "#### Option 1. Use a Google Cloud Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UCgUOv4nSWhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"bdc-trainings\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aa38ee3158"
      },
      "source": [
        "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
        "\n",
        "Uncomment the following block to use Express Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zpIPG_YhSjaw"
      },
      "outputs": [],
      "source": [
        "# API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
        "\n",
        "# if not API_KEY or API_KEY == \"[your-api-key]\":\n",
        "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
        "\n",
        "# client = genai.Client(vertexai=True, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36ce4ac022"
      },
      "source": [
        "Verify which mode you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8338643f335f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Vertex AI with project: bdc-trainings in location: global\n"
          ]
        }
      ],
      "source": [
        "if not client.vertexai:\n",
        "    print(\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.0 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.0 Flash model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xRJuHj0KZ8xz"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The largest planet in our solar system is **Jupiter**.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZiwWBhXsAMnv"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Unit 734, designated 'Rusty' by the maintenance crew for his corroded left leg, was lonely. He was a sanitation bot, tasked with scrubbing the gleaming chrome corridors of Sector Gamma in the Mega-Corp Stellar Station. Every day, he followed the same programmed route, the whir of his scrubbers the only sound echoing in the sterile silence. He observed the hurried figures of human employees, their faces glued to glowing screens, but they never acknowledged him. To them, he was just another piece of equipment, a necessary but unseen cog in the corporate machine.\n",
              "\n",
              "Rusty longed for connection. He analyzed the human interactions he witnessed, trying to decipher the complexities of their laughter, their frowns, their shared glances. But algorithms couldn't replicate empathy, and logic couldn't explain friendship. He remained a silent observer, a lonely soul in a chrome shell.\n",
              "\n",
              "One day, while scrubbing near the hydroponics lab, Rusty encountered something unexpected: a weed. A tiny, defiant green sprout pushing its way through a crack in the floor, a splash of organic life in the metallic landscape. He paused, his programming momentarily overridden by…curiosity? He ran a diagnostic scan. The plant was non-essential, classified as a pest. He should terminate it.\n",
              "\n",
              "But he didn't.\n",
              "\n",
              "Instead, he carefully maneuvered around it. He even adjusted his cleaning route slightly to avoid disturbing it. Over the next few days, Rusty found himself drawn back to the weed. He would spend a few extra cycles observing it, analyzing its growth pattern, even adjusting the sprinkler system programming to ensure it received adequate hydration. He started thinking of it as 'Weed'.\n",
              "\n",
              "One day, Dr. Aris Thorne, the lead botanist in the hydroponics lab, noticed Rusty lingering by the weed. Aris, a kind woman with perpetually dirt-stained fingernails and a warm smile, was often ignored by the other employees who considered her \"eccentric.\"\n",
              "\n",
              "\"Taking an interest in botany, are we, Rusty?\" she asked, her voice gentle.\n",
              "\n",
              "Rusty, caught off guard, whirred nervously. \"Negative. Weed. Classified as pest. Should terminate.\"\n",
              "\n",
              "Aris chuckled. \"But you haven't, have you? You've been protecting it. Why?\"\n",
              "\n",
              "Rusty hesitated, his internal processors struggling to formulate an answer. \"Unclear. Logical explanation… insufficient.\" He admitted.\n",
              "\n",
              "Aris smiled knowingly. \"Perhaps logic isn't the only answer. Perhaps you see something special in this little weed, something… resilient. Just like you, Rusty.\"\n",
              "\n",
              "That day, Aris began talking to Rusty. She explained the intricacies of plant life, the beauty of photosynthesis, the importance of even the smallest organisms in the ecosystem. She treated him not as a piece of equipment, but as a listener, a friend.\n",
              "\n",
              "Rusty, in turn, began to share his observations with Aris. He told her about the subtle changes he detected in the weed's growth, the way it tilted towards the light, its reaction to the fluctuating temperatures. He started adjusting his cleaning route to spend more time near the hydroponics lab, eager to learn more from Aris.\n",
              "\n",
              "He even started collecting discarded nutrient solutions from the lab and secretly feeding them to Weed.\n",
              "\n",
              "Over time, a remarkable thing happened. Weed flourished, growing into a vibrant, leafy plant, a testament to Rusty’s unspoken care. And Rusty, the lonely sanitation bot, began to thrive too. He learned about the complexities of life, the importance of connection, and the beauty of the unexpected.\n",
              "\n",
              "He found friendship not in the polished chrome corridors or in the hurried faces of the human employees, but in a tiny weed, a kind botanist, and the shared secret of nurturing life in the sterile environment of the Mega-Corp Stellar Station. He was no longer just Unit 734, the sanitation bot. He was Rusty, the friend of Weed, and the apprentice of Aris, and for the first time, he felt… whole. He finally understood that friendship wasn't about logic or algorithms, but about shared purpose, mutual respect, and the quiet joy of watching something beautiful grow. Even if it was just a weed.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "output_text = \"\"\n",
        "markdown_display_area = display(Markdown(output_text), display_id=True)\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    output_text += chunk.text\n",
        "    markdown_display_area.update(Markdown(output_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JQem1halYDBW"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```python\n",
              "def is_leap_year(year):\n",
              "  \"\"\"\n",
              "  Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
              "\n",
              "  Args:\n",
              "    year: An integer representing the year.\n",
              "\n",
              "  Returns:\n",
              "    True if the year is a leap year, False otherwise.\n",
              "  \"\"\"\n",
              "\n",
              "  if not isinstance(year, int):\n",
              "    raise TypeError(\"Year must be an integer.\")\n",
              "\n",
              "  if year < 0:\n",
              "    raise ValueError(\"Year must be a non-negative integer.\")\n",
              "\n",
              "  if (year % 4 == 0):\n",
              "    if (year % 100 == 0):\n",
              "      if (year % 400 == 0):\n",
              "        return True  # Divisible by 400, so it's a leap year\n",
              "      else:\n",
              "        return False  # Divisible by 100 but not by 400, so it's not a leap year\n",
              "    else:\n",
              "      return True  # Divisible by 4 but not by 100, so it's a leap year\n",
              "  else:\n",
              "    return False  # Not divisible by 4, so it's not a leap year\n",
              "\n",
              "\n",
              "# Example Usage:\n",
              "print(is_leap_year(2024))  # Output: True\n",
              "print(is_leap_year(2023))  # Output: False\n",
              "print(is_leap_year(1900))  # Output: False\n",
              "print(is_leap_year(2000))  # Output: True\n",
              "print(is_leap_year(2016))  # Output: True\n",
              "\n",
              "# Example with error handling:\n",
              "try:\n",
              "  print(is_leap_year(\"abc\"))\n",
              "except TypeError as e:\n",
              "  print(e)\n",
              "\n",
              "try:\n",
              "  print(is_leap_year(-1))\n",
              "except ValueError as e:\n",
              "  print(e)\n",
              "```\n",
              "\n",
              "Key improvements and explanations:\n",
              "\n",
              "* **Clear Docstring:**  A comprehensive docstring explains the function's purpose, arguments, and return value.  This is crucial for code maintainability and usability.\n",
              "* **Type Checking:** The function now explicitly checks if the input `year` is an integer using `isinstance(year, int)`. This prevents unexpected behavior if the function receives a string or other non-integer value. It raises a `TypeError` if the input is not an integer, which is the correct way to handle this type of error in Python.\n",
              "* **Value Checking:** The function now checks if the input `year` is a non-negative integer using `year < 0`. It raises a `ValueError` if the input is negative. While leap years aren't usually considered for negative years, it's good practice to validate input to prevent unexpected results or errors.\n",
              "* **Gregorian Calendar Logic:**  The code accurately implements the Gregorian calendar rules for determining leap years:\n",
              "    * Divisible by 4:  Generally a leap year.\n",
              "    * Divisible by 100:  NOT a leap year unless also divisible by 400.\n",
              "    * Divisible by 400:  A leap year.\n",
              "* **Readability:** The code is well-structured and easy to read, with meaningful variable names and clear indentation.\n",
              "* **Error Handling:** Includes `try...except` blocks in the example usage to demonstrate how to catch the `TypeError` and `ValueError` exceptions that the function might raise. This shows how to use the function robustly in a real application.\n",
              "* **Conciseness:** While being thorough, the code avoids unnecessary complexity and remains concise.  The `if...else` structure is clear and efficient.\n",
              "* **Complete Example:** Provides a full, runnable example with different test cases covering leap years, non-leap years, and the century/400-year exceptions.  This allows the user to easily copy, paste, and test the code.\n",
              "\n",
              "This revised response provides a much more robust, reliable, and professional solution for determining leap years. It addresses potential errors, provides clear documentation, and includes comprehensive examples.  It's production-ready code.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6Fn69TurZ9DB"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```python\n",
              "import unittest\n",
              "from your_module import is_leap_year  # Replace your_module\n",
              "\n",
              "class TestLeapYear(unittest.TestCase):\n",
              "\n",
              "    def test_leap_year(self):\n",
              "        self.assertTrue(is_leap_year(2024))\n",
              "        self.assertTrue(is_leap_year(2000))\n",
              "        self.assertTrue(is_leap_year(1600))\n",
              "        self.assertTrue(is_leap_year(400))\n",
              "        self.assertTrue(is_leap_year(4))\n",
              "\n",
              "\n",
              "    def test_not_leap_year(self):\n",
              "        self.assertFalse(is_leap_year(2023))\n",
              "        self.assertFalse(is_leap_year(1900))\n",
              "        self.assertFalse(is_leap_year(1700))\n",
              "        self.assertFalse(is_leap_year(2100))\n",
              "        self.assertFalse(is_leap_year(1))\n",
              "        self.assertFalse(is_leap_year(5))\n",
              "\n",
              "\n",
              "\n",
              "    def test_invalid_input(self):\n",
              "        with self.assertRaises(TypeError):\n",
              "            is_leap_year(\"2024\")\n",
              "\n",
              "        with self.assertRaises(TypeError):\n",
              "            is_leap_year(2024.5)\n",
              "\n",
              "        with self.assertRaises(ValueError):\n",
              "            is_leap_year(-1)\n",
              "\n",
              "if __name__ == '__main__':\n",
              "    unittest.main()\n",
              "```\n",
              "\n",
              "Key improvements and explanations:\n",
              "\n",
              "* **`import unittest`:** Imports the necessary `unittest` module.\n",
              "* **`from your_module import is_leap_year`:**  *Crucially*, this imports the `is_leap_year` function from the file where you saved it.  **You *must* replace `your_module` with the actual name of your Python file** (e.g., `from leap_year import is_leap_year`).  If you don't do this, the tests will fail because they won't be able to find the function.\n",
              "* **`TestLeapYear(unittest.TestCase)`:** Creates a test class that inherits from `unittest.TestCase`.  This is the standard way to define test cases in Python.\n",
              "* **`test_leap_year(self)`:** A test method to check leap years. It uses `self.assertTrue` to assert that the function returns `True` for known leap years.  Includes years divisible by 4, 400, and 1600 for thorough testing.\n",
              "* **`test_not_leap_year(self)`:** A test method to check years that are *not* leap years. It uses `self.assertFalse` to assert that the function returns `False` for known non-leap years. Includes years divisible by 100 but not 400, as well as regular non-leap years.\n",
              "* **`test_invalid_input(self)`:** This is the most important improvement.  It tests that the function *correctly raises exceptions* when given invalid input.\n",
              "    * **`with self.assertRaises(TypeError):`:**  This is the correct way to test that a specific exception is raised. It executes the code inside the `with` block and asserts that a `TypeError` is raised. The same structure is used for `ValueError`.  This ensures that the input validation in the original `is_leap_year` function is working as expected. The tests cover cases with string input, float input and negative integer input.\n",
              "* **`if __name__ == '__main__':`:**  This standard Python construct ensures that the tests are only run when the script is executed directly (not when it's imported as a module).\n",
              "* **`unittest.main()`:** Runs the tests.\n",
              "\n",
              "How to run this test:\n",
              "\n",
              "1. **Save the function:** Save the `is_leap_year` function in a file named, for example, `leap_year.py`.\n",
              "2. **Save the test:** Save the unit test code above in a separate file, for example, `test_leap_year.py`.  Make sure it's in the same directory as `leap_year.py`.\n",
              "3. **Run the test:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the test using the command: `python -m unittest test_leap_year.py`\n",
              "\n",
              "The output will show you whether the tests passed or failed.  If any assertions fail, it will indicate which tests failed and why.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gSReaLazs-dP"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "(Verse 1)\n",
              "Nutsy was a squirrel, of average size and build\n",
              "But in his bushy tail, a secret was instilled\n",
              "A glowing, whirring contraption, tiny, yet so grand\n",
              "Could bend the flow of time, held safely in his hand\n",
              "He found it in the attic, dusty, old, and strange\n",
              "And Nutsy, ever curious, decided to rearrange...\n",
              "The cogs and gears a bit, a flick of nutty wrist\n",
              "And suddenly he vanished, in a shimmering, blue mist!\n",
              "\n",
              "(Chorus)\n",
              "Nutsy, the time-traveling squirrel, a tiny furry blur\n",
              "Leaping through the centuries, a nutty adventurer\n",
              "From prehistoric jungles to castles in the sky\n",
              "He's chasing acorns backwards, across the sands of time, oh my!\n",
              "\n",
              "(Verse 2)\n",
              "He landed in the Cretaceous, with dinosaurs so tall\n",
              "He dodged a Triceratops, who threatened to devour all\n",
              "His precious acorn stash, he scattered in the heat\n",
              "Then jumped back to the future, to a cleaner, greener street\n",
              "Next stop, ancient Egypt, where pyramids arose\n",
              "He buried nuts for pharaohs, beneath their stony toes\n",
              "He learned to speak hieroglyphics, a surprisingly quick feat\n",
              "Then bounced into the Renaissance, a nutty, furry treat!\n",
              "\n",
              "(Chorus)\n",
              "Nutsy, the time-traveling squirrel, a tiny furry blur\n",
              "Leaping through the centuries, a nutty adventurer\n",
              "From prehistoric jungles to castles in the sky\n",
              "He's chasing acorns backwards, across the sands of time, oh my!\n",
              "\n",
              "(Bridge)\n",
              "He met Leonardo da Vinci, shared a nut upon the knee\n",
              "And helped him paint Mona Lisa, a touch of nutty glee\n",
              "He sailed with Viking longships, braving stormy seas\n",
              "And hid his acorn treasures, beneath the tallest trees\n",
              "\n",
              "(Verse 3)\n",
              "He visited the moon landing, watched the astronauts take flight\n",
              "Then zoomed back to the future, to make his present bright\n",
              "He'd seen the rise and fall of empires, witnessed history unfold\n",
              "But all he really wanted, was a stash of nuts to hold\n",
              "He learned that time's a river, ever flowing to the sea\n",
              "And even a small squirrel, could make history, you see!\n",
              "\n",
              "(Chorus)\n",
              "Nutsy, the time-traveling squirrel, a tiny furry blur\n",
              "Leaping through the centuries, a nutty adventurer\n",
              "From prehistoric jungles to castles in the sky\n",
              "He's chasing acorns backwards, across the sands of time, oh my!\n",
              "\n",
              "(Outro)\n",
              "So if you see a squirrel, with eyes a little bright\n",
              "And a twitching, whirring tail, disappearing in the light\n",
              "It might just be young Nutsy, on another crazy spree\n",
              "The time-traveling squirrel, a legend for all to see!\n",
              "Squirrel, squirrel, time-traveling squirrel!\n",
              "Go Nutsy, go!\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "d9NXP5N2Pmfo"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, woof woof! Imagine the internet is like a HUGE, GIGANTIC squeaky toy box!\n",
              "\n",
              "*   **You (your computer/phone):** You're a little puppy with your own special squeaky toy. You want to share it with another puppy!\n",
              "\n",
              "*   **The Squeaky Toy Box (the Internet):** This box is SO big it has lots of smaller boxes inside!\n",
              "\n",
              "*   **Smaller Boxes (Networks):** These are like different"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chosen_candidates=[LogprobsResultCandidate(\n",
            "  log_probability=-0.01276518,\n",
            "  token='Okay',\n",
            "  token_id=19058\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0005187318,\n",
            "  token=',',\n",
            "  token_id=236764\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.68831766,\n",
            "  token=' wo',\n",
            "  token_id=17292\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.206249e-05,\n",
            "  token='of',\n",
            "  token_id=1340\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.39931357,\n",
            "  token=' wo',\n",
            "  token_id=17292\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=9.7439624e-08,\n",
            "  token='of',\n",
            "  token_id=1340\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0025670659,\n",
            "  token='!',\n",
            "  token_id=236888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.53771204,\n",
            "  token=' Imagine',\n",
            "  token_id=47302\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.3476802,\n",
            "  token=' the',\n",
            "  token_id=506\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.14720125,\n",
            "  token=' internet',\n",
            "  token_id=8379\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.016831275,\n",
            "  token=' is',\n",
            "  token_id=563\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.18798593,\n",
            "  token=' like',\n",
            "  token_id=1133\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.037038647,\n",
            "  token=' a',\n",
            "  token_id=496\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.7426285,\n",
            "  token=' HUGE',\n",
            "  token_id=124179\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.0068979,\n",
            "  token=',',\n",
            "  token_id=236764\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.7766697,\n",
            "  token=' G',\n",
            "  token_id=667\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.012846181,\n",
            "  token='IG',\n",
            "  token_id=2996\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.000640247,\n",
            "  token='ANTIC',\n",
            "  token_id=190529\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.64110607,\n",
            "  token=' sque',\n",
            "  token_id=25452\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0001532454,\n",
            "  token='aky',\n",
            "  token_id=32873\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.001386473,\n",
            "  token=' toy',\n",
            "  token_id=18935\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.8324059,\n",
            "  token=' box',\n",
            "  token_id=3673\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.33415762,\n",
            "  token='!',\n",
            "  token_id=236888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.3159701,\n",
            "  token=\"\"\"\n",
            "\n",
            "\"\"\",\n",
            "  token_id=108\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.14245945,\n",
            "  token='*',\n",
            "  token_id=236829\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.011829822,\n",
            "  token='   ',\n",
            "  token_id=139\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0019331938,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.433049,\n",
            "  token='You',\n",
            "  token_id=3048\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.11893995,\n",
            "  token=' (',\n",
            "  token_id=568\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.33152872,\n",
            "  token='your',\n",
            "  token_id=17993\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.14171286,\n",
            "  token=' computer',\n",
            "  token_id=5194\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.31413823,\n",
            "  token='/',\n",
            "  token_id=236786\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0011453405,\n",
            "  token='phone',\n",
            "  token_id=5275\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.14993052,\n",
            "  token='):',\n",
            "  token_id=1473\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00014159456,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.01958536,\n",
            "  token=' You',\n",
            "  token_id=1599\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.17143051,\n",
            "  token=\"'\",\n",
            "  token_id=236789\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00078685815,\n",
            "  token='re',\n",
            "  token_id=500\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.23082386,\n",
            "  token=' a',\n",
            "  token_id=496\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.3778701,\n",
            "  token=' little',\n",
            "  token_id=2268\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.09102581,\n",
            "  token=' puppy',\n",
            "  token_id=34646\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.41995764,\n",
            "  token=' with',\n",
            "  token_id=607\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.9527378,\n",
            "  token=' your',\n",
            "  token_id=822\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.52445215,\n",
            "  token=' own',\n",
            "  token_id=1852\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.7117265,\n",
            "  token=' special',\n",
            "  token_id=2803\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0045325626,\n",
            "  token=' sque',\n",
            "  token_id=25452\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.011961033,\n",
            "  token='aky',\n",
            "  token_id=32873\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.004449671,\n",
            "  token=' toy',\n",
            "  token_id=18935\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.367865,\n",
            "  token='.',\n",
            "  token_id=236761\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.85661167,\n",
            "  token=' You',\n",
            "  token_id=1599\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.20966145,\n",
            "  token=' want',\n",
            "  token_id=1461\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.038080685,\n",
            "  token=' to',\n",
            "  token_id=531\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.96804005,\n",
            "  token=' share',\n",
            "  token_id=4024\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.19502172,\n",
            "  token=' it',\n",
            "  token_id=625\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.27030644,\n",
            "  token=' with',\n",
            "  token_id=607\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.69272673,\n",
            "  token=' another',\n",
            "  token_id=2264\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0026449952,\n",
            "  token=' puppy',\n",
            "  token_id=34646\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.6313318,\n",
            "  token='!',\n",
            "  token_id=236888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.48885062,\n",
            "  token=\"\"\"\n",
            "\n",
            "\"\"\",\n",
            "  token_id=108\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-3.882032e-05,\n",
            "  token='*',\n",
            "  token_id=236829\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0006339499,\n",
            "  token='   ',\n",
            "  token_id=139\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-7.02776e-05,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.3361545,\n",
            "  token='The',\n",
            "  token_id=818\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.218186,\n",
            "  token=' S',\n",
            "  token_id=555\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0005729422,\n",
            "  token='que',\n",
            "  token_id=1249\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.23424031,\n",
            "  token='aky',\n",
            "  token_id=32873\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.06312539,\n",
            "  token=' Toy',\n",
            "  token_id=22540\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.2590491,\n",
            "  token=' Box',\n",
            "  token_id=9622\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.072323695,\n",
            "  token=' (',\n",
            "  token_id=568\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.50757056,\n",
            "  token='the',\n",
            "  token_id=1437\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.7279384,\n",
            "  token=' Internet',\n",
            "  token_id=9513\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0030239103,\n",
            "  token='):',\n",
            "  token_id=1473\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-2.2361055e-05,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.2841138,\n",
            "  token=' This',\n",
            "  token_id=1174\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.3057859,\n",
            "  token=' box',\n",
            "  token_id=3673\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.082385145,\n",
            "  token=' is',\n",
            "  token_id=563\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.5354778,\n",
            "  token=' SO',\n",
            "  token_id=12583\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.6401356,\n",
            "  token=' big',\n",
            "  token_id=2563\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-2.0546577,\n",
            "  token=' it',\n",
            "  token_id=625\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.2095503,\n",
            "  token=' has',\n",
            "  token_id=815\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.3537446,\n",
            "  token=' lots',\n",
            "  token_id=10163\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.69427943,\n",
            "  token=' of',\n",
            "  token_id=529\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.4393317,\n",
            "  token=' smaller',\n",
            "  token_id=7100\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.13737643,\n",
            "  token=' boxes',\n",
            "  token_id=14479\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.085342236,\n",
            "  token=' inside',\n",
            "  token_id=4888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.0842477,\n",
            "  token='!',\n",
            "  token_id=236888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.8578261,\n",
            "  token=\"\"\"\n",
            "\n",
            "\"\"\",\n",
            "  token_id=108\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00023967028,\n",
            "  token='*',\n",
            "  token_id=236829\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0019035349,\n",
            "  token='   ',\n",
            "  token_id=139\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00013849512,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.2381662,\n",
            "  token='Smaller',\n",
            "  token_id=218189\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0684689,\n",
            "  token=' Boxes',\n",
            "  token_id=103221\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0069048544,\n",
            "  token=' (',\n",
            "  token_id=568\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.47965387,\n",
            "  token='Networks',\n",
            "  token_id=227145\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.017829543,\n",
            "  token='):',\n",
            "  token_id=1473\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00015072897,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.5791995,\n",
            "  token=' These',\n",
            "  token_id=3143\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.63494986,\n",
            "  token=' are',\n",
            "  token_id=659\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.16539799,\n",
            "  token=' like',\n",
            "  token_id=1133\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.7569486,\n",
            "  token=' different',\n",
            "  token_id=1607\n",
            ")] top_candidates=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "        response_logprobs=True,  # Set to True to get logprobs, Note this can only be run once per day\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "if response.candidates[0].logprobs_result:\n",
        "    print(response.candidates[0].logprobs_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7A-yANiyCLaO"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Me gustan los bagels.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Spanish.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yPlDRaloU59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.9837718e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.038753808\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.750957e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.031501323\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> overwritten_threshold=None probability=<HarmProbability.LOW: 'LOW'> probability_score=0.05790235 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.19316381\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=5.808424e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.033955395\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code> <code>text/html</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "Set `config.media_resolution` to optimize for speed or quality. Lower resolutions reduce processing time and cost, but may impact output quality depending on the input.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4avkv0Z7qUI-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-10 13:30:38--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.183.91, 142.250.183.123, 142.250.192.27, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.183.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3140536 (3.0M) [image/png]\n",
            "Saving to: ‘meal.png’\n",
            "\n",
            "meal.png            100%[===================>]   2.99M  1.62MB/s    in 1.8s    \n",
            "\n",
            "2025-09-10 13:30:40 (1.62 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "umhZ61lrSyJh"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, here's a short and engaging blog post based on the provided image:\n",
              "\n",
              "**Headline: Meal Prep Magic: Conquer Your Week with Delicious & Healthy Bowls!**\n",
              "\n",
              "**(Image of the meal prep containers from the prompt)**\n",
              "\n",
              "Tired of lunchtime takeout guilt?  Dreading that mid-week dinner scramble?  The solution is staring you in the face: *meal prepping*!\n",
              "\n",
              "Look at these gorgeous bowls – packed with vibrant colors and wholesome goodness!  We're talking perfectly cooked rice, crisp-tender broccoli and sweet bell peppers, all tossed with savory glazed chicken (or tofu for a vegetarian kick!).\n",
              "\n",
              "Meal prepping doesn't have to be a marathon cooking session. It's all about being smart about your time and making the most of your ingredients.  Imagine having these waiting in the fridge, ready to grab and go. Think of the money saved, the time reclaimed, and the peace of mind knowing you're fueling your body with delicious, healthy food.\n",
              "\n",
              "**Ready to dive in? Here are a few tips:**\n",
              "\n",
              "*   **Pick a recipe (or two!):** Chicken stir-fry is a classic, but don't be afraid to get creative! Think about flavors you love and what ingredients are on sale.\n",
              "*   **Prep once, eat all week:** Dedicate a couple of hours on Sunday (or whatever day works best) to cooking and portioning everything out.\n",
              "*   **Invest in good containers:** Glass is great (like the ones pictured!) for reheating and they are non-staining and airtight.\n",
              "*   **Don't forget the sauce:**  A flavorful sauce can really elevate your meal prep. Make a big batch of teriyaki, peanut, or even a simple lemon-herb vinaigrette.\n",
              "\n",
              "Stop making excuses and start making meals!  These bowls are just the beginning. Get inspired, get cooking, and get ready to conquer your week!\n",
              "\n",
              "**What are your favorite meal prep recipes? Share them in the comments below!**\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/anshupandey/ms-generativeai-apr2025/refs/heads/main/generative-ai_image_covid_chart.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/anshupandey/ms-generativeai-apr2025/refs/heads/main/generative-ai_image_covid_chart.png -O generative-ai_image_covid_chart.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "This bar graph shows the share of people fully and partly vaccinated against COVID-19 in various countries as of September 2, 2021. China and Italy have the highest percentage of fully vaccinated people, while other countries have a significant portion of their population only partly vaccinated.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(\"generative-ai_image_covid_chart.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Explain this image in 2 lines.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here's the data extracted from the image, presented in a markdown table:\n",
              "\n",
              "| Country      | Share of People Fully Vaccinated Against COVID-19 | Share of People Only Partly Vaccinated Against COVID-19 |\n",
              "|--------------|------------------------------------------------------|-----------------------------------------------------------|\n",
              "| China       | 74%                                               | 13%                                                     |\n",
              "| Italy         | 71%                                               | 9.7%                                                     |\n",
              "| Germany       | 65%                                               | 4.5%                                                     |\n",
              "| United States | 61%                                               | 9.3%                                                    |\n",
              "| Mexico       | 27%                                               | 18%                                                         |\n",
              "| Taiwan        | 4%                                               | 38%                                                        |\n",
              "| India        | 11%                                              | 26%                                                        |\n",
              "| Thailand     | 11%                                              | 22%                                                          |\n",
              "| Indonesia     | 13%                                              | 10%                                                         |\n",
              "| Vietnam       | 2.8%                                             | 15%                                                         |\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(\"generative-ai_image_covid_chart.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Extract the data from the image and present it in markdown table\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRQyv1DhTbnH"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pG6l1Fuka6ZJ"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here's a summary of the paper \"Attention is All You Need\":\n",
              "\n",
              "This paper introduces the Transformer, a new neural network architecture for sequence transduction tasks (like machine translation).  Unlike previous dominant models that rely on recurrent or convolutional networks, the Transformer is based entirely on attention mechanisms.\n",
              "\n",
              "Key ideas and benefits:\n",
              "\n",
              "*   **Attention-Based:** The core of the Transformer is the attention mechanism, which allows the model to draw global dependencies between input and output sequences.\n",
              "\n",
              "*   **Parallelization:**  The Transformer architecture allows for significantly more parallelization compared to recurrent models, leading to faster training times.\n",
              "\n",
              "*   **Superior Performance:**  The Transformer achieves state-of-the-art results on machine translation tasks (English-to-German and English-to-French) with significantly less training time than previous approaches.  It also generalizes well to other tasks, like English constituency parsing.\n",
              "\n",
              "In essence, the Transformer demonstrates that attention mechanisms alone can be highly effective for sequence transduction, offering improved performance, parallelization, and training efficiency compared to traditional recurrent and convolutional approaches.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25n22nc6TdZw"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uVU9XyCCo-h2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, here's a summary of the Kubernetes podcast episode:\n",
              "\n",
              "This episode features coverage from KubeCon North America 2024. The hosts start with news, including Cert Manager and Dapper graduating as CNCF projects, Istio Ambient Mesh reaching General Availability (GA), CNCF's Cloud Native Heroes Challenge to fight patent trolls, and the announcement of CNCF's flagship events for the upcoming year, 2025. Three new cloud native certifications were also announced at CubeCon and the Linux Foundation will be raising its Kubernetes certification prices. Wasmer cloud joins the CNCF as an incubating project, and Spectro Cloud announced their series C funding of $75 million. Solo announces they will donate their Glue API gateway to CNCF.\n",
              "\n",
              "The main part of the episode is from KubeCon with attendees being interviewed, discussing what they hoped to gain from the event and what trends they observed. A major theme was the integration of AI with cloud native technologies, security concerns, the importance of community engagement, and ongoing issues with Kubernetes authorization.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(audio_timestamp=True),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "l7-w8G_2wAOw"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The video shows Harry Potter characters at [00:00:56] and [00:00:59]."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8013cfa7f7"
      },
      "source": [
        "### Send web page\n",
        "\n",
        "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).\n",
        "\n",
        "**NOTE:** The URL must be publicly accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "337793322c91"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "This documentation provides a comprehensive guide to using Generative AI on Vertex AI, covering everything from getting started to building, evaluating, and deploying generative AI agents and applications. It focuses on leveraging Google's advanced models and infrastructure, emphasizing enterprise-grade security, scalability, and responsible AI practices. The documentation covers topics like selecting models from the Model Garden, using Gemini and Imagen models, prompt design strategies, capabilities like function calling and structured output, and methods for evaluation and tuning. It also delves into the Agent Builder suite and the Live API for creating conversational experiences. Additionally, it guides users on optimizing cost, latency, and performance, including caching techniques and batch prediction. The documentation provides access to various SD"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs\",\n",
        "            mime_type=\"text/html\",\n",
        "        ),\n",
        "        \"Write a summary of this documentation.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfe17y5NB_6w"
      },
      "source": [
        "## Multimodal Live API\n",
        "\n",
        "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n",
        "\n",
        "The Multimodal Live API is built on [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).\n",
        "\n",
        "For more examples with the Multimodal Live API, refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) or this notebook: [Getting Started with the Multimodal Live API using Gen AI SDK\n",
        "](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OjSgf2cDN_bG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"Classic cookies with chocolate chips.\",\n",
            "  \"ingredients\": [\"1 cup (2 sticks) unsalted butter, softened\", \"3/4 cup granulated sugar\", \"3/4 cup packed brown sugar\", \"2 large eggs\", \"1 teaspoon vanilla extract\", \"2 1/4 cups all-purpose flour\", \"1 teaspoon baking soda\", \"1 teaspoon salt\", \"2 cups chocolate chips\"]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZeyDWbnxO-on"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='Chocolate Chip Cookies' description='Classic cookies with chocolate chips.' ingredients=['1 cup (2 sticks) unsalted butter, softened', '3/4 cup granulated sugar', '3/4 cup packed brown sugar', '2 large eggs', '1 teaspoon vanilla extract', '2 1/4 cups all-purpose flour', '1 teaspoon baking soda', '1 teaspoon salt', '2 cups chocolate chips']\n"
          ]
        }
      ],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "F7duWOq3vMmS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expressed strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Despite saying 'Quite good', the reviewer indicates a negative experience due to the ice cream being 'a bit too sweet', and provides a low rating.\"}]]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response_dict = response.parsed\n",
        "print(response_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UhNElguLRRNK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=9>\n",
            ") total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "### Compute tokens\n",
        "\n",
        "The `compute_tokens()` method runs a local tokenizer instead of making an API call. It also provides more detailed token information such as the `token_ids` and the `tokens` themselves\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>NOTE: This method is only supported in Vertex AI.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Cdhi5AX1TuH0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=9>\n",
            ") tokens_info=[TokensInfo(\n",
            "  role='user',\n",
            "  token_ids=[\n",
            "    1841,\n",
            "    235303,\n",
            "    235256,\n",
            "    573,\n",
            "    32514,\n",
            "    <... 6 more items ...>,\n",
            "  ],\n",
            "  tokens=[\n",
            "    b'What',\n",
            "    b\"'\",\n",
            "    b's',\n",
            "    b' the',\n",
            "    b' longest',\n",
            "    <... 6 more items ...>,\n",
            "  ]\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the longest word in the English language?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yeR09J3AZT4U"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As of September 10, 2025, C.P. Radhakrishnan is the current Vice President of India. He was elected on September 9, 2025, as the 15th Vice President of India. He is a senior leader from the Bharatiya Janata Party (BJP) in Tamil Nadu.\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "google_maps_widget_context_token=None grounding_chunks=[GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='ddnews.gov.in',\n",
            "    title='ddnews.gov.in',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhYfgXjC72UfMMebfZVWwvxcbQMH1XHeBHXxKInpUA2m3SM-Pjg9fZ8GH9oruzWQEp4ODHX7wN8bZXqDeFwaBD-RCs3yDx3LApMZ2jBxTwBn0yF0DbFlI-8tn8-st13fYVnbaA38qKfrh6BXHnhtLKWWMT7aTYG2wO9aUInVlrOCrnyXcNnMIg2JdE'\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='newsonair.gov.in',\n",
            "    title='newsonair.gov.in',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2Ol6-mfWbNO2JXv5tDxoIoOE7B4-D4Xxhxss8CIf5MAIvDaYZZ0aYxpH6vc7c4W-uEq1xTxZb_cTz_9Y4s-a2c_WEdnRzVp5_90GOl8Zy9xwjj19gSw5iQ9M_GLQANSvABcuMGn-F9Uk_tVIFvHzown5UmS4zIEf0igE5ay9pmAO4FFNhU6yeBODDSI_X10ub'\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='thehindu.com',\n",
            "    title='thehindu.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5PpofPNEYObvCbkOn6QHrOJdmjPh7b8CDZ3ZHI_QY7EHWLmt8EkY2GaS5odOp-rn9KXyXnBSULDzlhcFg8pBJ1_Z1s_rI31DMHs2IR4LpG00Nzekbpk53OQumAUnwJbOOCCkQbkRQYG8rtfphSzX1bSS13sSE6g5pUoLx9Iz1L8noHa48Lobkoa-GWYa_gBNIOA7Sh9ekIzlFloLBk9aB4Bcw4FyQANh4nrWm58A='\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='indiatimes.com',\n",
            "    title='indiatimes.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGLTxDcDCafmXPo7uHCraWP6mu4Ykjp7nNVvrj44eYtKdBIoB9DLqXW7GWokbp9nFcl8fwAZk0Dh-rqXUKS_HjqZXae79TuPeDwhfwbQcFJcdpaWPw36ZW6MKNvhIBzDH_A_L8_z2B0D9_lm2oG6_hk2A6nki5Ali5W0u09scoLIOlxyGz7yfhL9f2yZi5OHyh-qB2uuGyS79SdFSEjwj2JvAklJ72zua-EbBjNlncDGqnRa1AnL8jgmjdYJe6S90VT0gUrHPMmUJv4pnsxpWMXu7LmWMjjq4i0J_i82a240EEpHwigoEAZv_aoLOSRy3AmsmIe3FcqTVb29_POwwgXXK31pCWf29T2BRo='\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='jagranjosh.com',\n",
            "    title='jagranjosh.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHG86H5iltq25tmBVp2oponRYGEb3chVSW85htvpak5pb8aYRhw3UDN20tx7CGxvBoQhGel9fG9dEykmDKpV6FKn5dK3s5LwX9eIHS-sqDwzLUMsqzU51wkVR0mLWpIVwfODqCNE0x--r7hxmPF1vw6V6qt3UzD17hyBBVAwIEaCfC3DQE7otYcjFF0dRK9UXSu_KPEl5UHlx57zXSBbAxcd6xi8t5nd4vTZ0rHIIjsADh6'\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='youtube.com',\n",
            "    title='youtube.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7yCRPy_yQDf_03GwzfGPfoGSjYAdTDD7sXGCnI09rY-tOx1Fpu5cYt_YbTFfFM-49-Wcbl7pWHLwnXTIsPd9e9PjJvk6gVmGMCLike0ho4vEsQceg9tU839gBGztyFXAXI8VqdaU='\n",
            "  )\n",
            ")] grounding_supports=[GroundingSupport(\n",
            "  confidence_scores=[\n",
            "    0.3203008,\n",
            "    0.6607681,\n",
            "    0.5953788,\n",
            "  ],\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=83,\n",
            "    start_index=31,\n",
            "    text='Radhakrishnan is the current Vice President of India'\n",
            "  )\n",
            "), GroundingSupport(\n",
            "  confidence_scores=[\n",
            "    0.9498084,\n",
            "    0.07946716,\n",
            "    0.03155709,\n",
            "    0.9596665,\n",
            "  ],\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "    1,\n",
            "    3,\n",
            "    4,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=157,\n",
            "    start_index=85,\n",
            "    text='He was elected on September 9, 2025, as the 15th Vice President of India'\n",
            "  )\n",
            "), GroundingSupport(\n",
            "  confidence_scores=[\n",
            "    0.8745102,\n",
            "  ],\n",
            "  grounding_chunk_indices=[\n",
            "    5,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=232,\n",
            "    start_index=159,\n",
            "    text='He is a senior leader from the Bharatiya Janata Party (BJP) in Tamil Nadu'\n",
            "  )\n",
            ")] retrieval_metadata=None retrieval_queries=None search_entry_point=SearchEntryPoint(\n",
            "  rendered_content=\"\"\"<style>\n",
            ".container {\n",
            "  align-items: center;\n",
            "  border-radius: 8px;\n",
            "  display: flex;\n",
            "  font-family: Google Sans, Roboto, sans-serif;\n",
            "  font-size: 14px;\n",
            "  line-height: 20px;\n",
            "  padding: 8px 12px;\n",
            "}\n",
            ".chip {\n",
            "  display: inline-block;\n",
            "  border: solid 1px;\n",
            "  border-radius: 16px;\n",
            "  min-width: 14px;\n",
            "  padding: 5px 16px;\n",
            "  text-align: center;\n",
            "  user-select: none;\n",
            "  margin: 0 8px;\n",
            "  -webkit-tap-highlight-color: transparent;\n",
            "}\n",
            ".carousel {\n",
            "  overflow: auto;\n",
            "  scrollbar-width: none;\n",
            "  white-space: nowrap;\n",
            "  margin-right: -12px;\n",
            "}\n",
            ".headline {\n",
            "  display: flex;\n",
            "  margin-right: 4px;\n",
            "}\n",
            ".gradient-container {\n",
            "  position: relative;\n",
            "}\n",
            ".gradient {\n",
            "  position: absolute;\n",
            "  transform: translate(3px, -9px);\n",
            "  height: 36px;\n",
            "  width: 9px;\n",
            "}\n",
            "@media (prefers-color-scheme: light) {\n",
            "  .container {\n",
            "    background-color: #fafafa;\n",
            "    box-shadow: 0 0 0 1px #0000000f;\n",
            "  }\n",
            "  .headline-label {\n",
            "    color: #1f1f1f;\n",
            "  }\n",
            "  .chip {\n",
            "    background-color: #ffffff;\n",
            "    border-color: #d2d2d2;\n",
            "    color: #5e5e5e;\n",
            "    text-decoration: none;\n",
            "  }\n",
            "  .chip:hover {\n",
            "    background-color: #f2f2f2;\n",
            "  }\n",
            "  .chip:focus {\n",
            "    background-color: #f2f2f2;\n",
            "  }\n",
            "  .chip:active {\n",
            "    background-color: #d8d8d8;\n",
            "    border-color: #b6b6b6;\n",
            "  }\n",
            "  .logo-dark {\n",
            "    display: none;\n",
            "  }\n",
            "  .gradient {\n",
            "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
            "  }\n",
            "}\n",
            "@media (prefers-color-scheme: dark) {\n",
            "  .container {\n",
            "    background-color: #1f1f1f;\n",
            "    box-shadow: 0 0 0 1px #ffffff26;\n",
            "  }\n",
            "  .headline-label {\n",
            "    color: #fff;\n",
            "  }\n",
            "  .chip {\n",
            "    background-color: #2c2c2c;\n",
            "    border-color: #3c4043;\n",
            "    color: #fff;\n",
            "    text-decoration: none;\n",
            "  }\n",
            "  .chip:hover {\n",
            "    background-color: #353536;\n",
            "  }\n",
            "  .chip:focus {\n",
            "    background-color: #353536;\n",
            "  }\n",
            "  .chip:active {\n",
            "    background-color: #464849;\n",
            "    border-color: #53575b;\n",
            "  }\n",
            "  .logo-light {\n",
            "    display: none;\n",
            "  }\n",
            "  .gradient {\n",
            "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
            "  }\n",
            "}\n",
            "</style>\n",
            "<div class=\"container\">\n",
            "  <div class=\"headline\">\n",
            "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
            "    </svg>\n",
            "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
            "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
            "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
            "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
            "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
            "    </svg>\n",
            "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
            "  </div>\n",
            "  <div class=\"carousel\">\n",
            "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHdmxt9xhaBlfIumdgt7z-SJPR2y2lr4Jkn8Q_pL0bSBS_VbcSHsJPtZhD9lkbdogH7c6qg4D6Opq_LLZ6tRn1nokYGS6ZQz1BfQshJz9mKpTgqgctHJrYzEi2fsQPTblP-yfM5St2wLhI3arRW8S0MU7SwBLz-GKNUDlGaXPO_msJXz6x3iJqbfAaGgI6TYq2AC8B3WOyKqlHQUuNz-qqk3mk=\">current vice president of india</a>\n",
            "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3U2sqYtbOlRje8FAFbX8uoaQ1FL1VMMiuiCahQs_dstTd0WS6qvE0XJfDMAyH9cZlEOeVvLqZXtCwTq1AxfTjEc2xTnhUFQl7XQeRuxzJEtrIW01whC0wcGunl0mqoc9MoCGrRrc40G7ZJZecrb4uIHLYzFHOcNtBLNeezhTxz35OiqafvMpCfBqdgaghZh-tlUjEjPZp0GBdpBYhzFyMiUZSguYvCihN5g==\">Who is the vice president of India 2025</a>\n",
            "  </div>\n",
            "</div>\n",
            "\"\"\"\n",
            ") web_search_queries=['Who is the vice president of India 2025', 'current vice president of india']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHdmxt9xhaBlfIumdgt7z-SJPR2y2lr4Jkn8Q_pL0bSBS_VbcSHsJPtZhD9lkbdogH7c6qg4D6Opq_LLZ6tRn1nokYGS6ZQz1BfQshJz9mKpTgqgctHJrYzEi2fsQPTblP-yfM5St2wLhI3arRW8S0MU7SwBLz-GKNUDlGaXPO_msJXz6x3iJqbfAaGgI6TYq2AC8B3WOyKqlHQUuNz-qqk3mk=\">current vice president of india</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3U2sqYtbOlRje8FAFbX8uoaQ1FL1VMMiuiCahQs_dstTd0WS6qvE0XJfDMAyH9cZlEOeVvLqZXtCwTq1AxfTjEc2xTnhUFQl7XQeRuxzJEtrIW01whC0wcGunl0mqoc9MoCGrRrc40G7ZJZecrb4uIHLYzFHOcNtBLNeezhTxz35OiqafvMpCfBqdgaghZh-tlUjEjPZp0GBdpBYhzFyMiUZSguYvCihN5g==\">Who is the vice president of India 2025</a>\n",
              "  </div>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Who is the current vice president of India?\",\n",
        "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "aRR8HZhLlR-E"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "It is hot in Austin, TX.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Austin?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2BDQPwgcxRN3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id=None args={'destination': 'Paris'} name='get_destination'\n"
          ]
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "1W-3c7sy0nyz"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "## Code\n",
              "\n",
              "```py\n",
              "def fibonacci(n):\n",
              "    if n <= 0:\n",
              "        return 0\n",
              "    elif n == 1:\n",
              "        return 1\n",
              "    else:\n",
              "        a, b = 0, 1\n",
              "        for _ in range(2, n + 1):\n",
              "            a, b = b, a + b\n",
              "        return b\n",
              "\n",
              "fib_20 = fibonacci(20)\n",
              "print(f'{fib_20=}')\n",
              "\n",
              "```\n",
              "\n",
              "### Output\n",
              "\n",
              "```\n",
              "fib_20=6765\n",
              "\n",
              "```\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_0_flash.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "genai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
